---
title: "Nested Model Comparison in General and Generalized Linear Models"
author: |
  | Mark Andrews
  | Psychology Department, Nottingham Trent University
  | 
  | \faEnvelopeO\  ```mark.andrews@ntu.ac.uk```
fontsize: 10pt
output:
 beamer_presentation:
  keep_tex: true
  fonttheme: "serif"
  includes:
   in_header: preamble.tex
bibliography: mjandrews.bib
biblio-style: apalike     
editor_options: 
  chunk_output_type: console
---


```{r, echo=F}
knitr::opts_chunk$set(echo = F, prompt = F, warning = F, message = F, comment='#>')
# Thanks to 
# https://github.com/ramnathv/slidify/issues/189#issuecomment-15850008
hook1 <- function(x){ gsub("```\n*```r*\n*", "", x) }
hook2 <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = hook1)
```

```{r}
library(tidyverse)
library(modelr)
library(here)
theme_set(theme_classic())

source(here('utils/utils.R'))
```

# What are nested models

* Model $M_0$ is nested in model $M_1$ if the parameter space of $M_0$ is a subset of the parameter space of $M_1$.
* For example, if $M_0$ is the following linear model:
$$
\text{for $i \in 1\ldots n,$} \quad y_i = \beta_0 + \beta_1 x_{1i} + \epsilon_i,\quad \epsilon_i \sim N(0, \sigma^2),
$$
its parameter space is $\beta_0$, $\beta_1$, $\sigma^2$.
* If $M_1$ is the following linear model:
$$
\text{for $i \in 1\ldots n,$} \quad y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i,\quad \epsilon_i \sim N(0, \sigma^2),
$$
its parameter space is $\beta_0$, $\beta_1$, $\beta_2$, $\sigma^2$.
* Any set of values of  $\beta_0$, $\beta_1$, $\sigma^2$ in $M_0$ is a point in the parameter space of $\beta_0$, $\beta_1$, $\beta_2$, $\sigma^2$ of $M_1$ if we simply set $\beta_2 = 0$.
* In other words, we can make $M_0$ with any given values of $\beta_0$, $\beta_1$, $\sigma^2$ from $M_1$ by setting $\beta_0$, $\beta_1$, $\sigma^2$ in $M_1$ to these same values and setting $\beta_2 = 0$.

# Nested normal linear models

* We can compare nested normal linear models using F tests.
* Assume $M_0$ and $M_1$ are normal linear models, with $M_0$ nested in $M_1$.
* We calculate $\text{RSS}_0$ and $\text{RSS}_1$, the residual sums of squares of $M_0$ and $M_1$, respectively.
* $\text{RSS}_0$ will be greater than or equal to $\text{RSS}_1$.
* Then
$$
\begin{aligned}
\text{proportional increase in error}&= \frac{\text{increase in error}}{\text{minimal error}} ,\\
\\
&= \frac{\text{RSS}_0 - \text{RSS}_1}{\text{RSS}_1},
\end{aligned}
$$

# Nested normal linear models

```{r, echo=T}
M1 <- lm(Fertility ~ Agriculture + Education + Catholic, data = swiss)
M0 <- lm(Fertility ~ Agriculture + Education, data = swiss)

RSS_0 <- sum(residuals(M0)^2)
RSS_1 <- sum(residuals(M1)^2)

c(RSS_0, RSS_1)
```

```{r, echo=T}
(RSS_0 - RSS_1)/RSS_1
```

In other words, $\text{RSS}_0$ is `r round(1 + ((RSS_0 - RSS_1)/RSS_1), 2)` greater than $\text{RSS}_1$.

# Nested normal linear models

* The F ratio is 
$$
\begin{aligned}
F &= \underbrace{\frac{\text{RSS}_0 - \text{RSS}_1}{\text{RSS}_1}}_{\text{effect size}} \times \underbrace{\frac{\text{df}_1}{\text{df}_0 - \text{df}_1}}_{\text{sample size}}
  &= \frac{(\text{RSS}_0 - \text{RSS}_1)/(\text{df}_0 - \text{df}_1)}{\text{RSS}_1/\text{df}_1}.
\end{aligned}
$$
where $\text{df}_1$ is $N - (K_1 + 1)$, where $K_1$ is number of (predictor; excluding intercept) coefficients in $M_1$.

```{r, echo=T}
df_0 <- M0$df.residual
df_1 <- M1$df.residual
c(df_0, df_1, df_0 - df_1, df_1/(df_0 - df_1))
```


# Nested normal linear models

```{r, echo=T}
RSS_0
RSS_1
RSS_0 - RSS_1
df_0 - df_1
df_1
(RSS_0 - RSS_1)/(df_0 - df_1)
RSS_1/df_1
((RSS_0 - RSS_1)/(df_0 - df_1))/(RSS_1/df_1)
```

# Nested normal linear models

```{r, echo=T}
anova(M0, M1)
```

# Nested normal linear models

```{r, echo=T}
drop1(M1, scope = ~ Catholic, test = 'F')
```

# Nested normal linear models

```{r, echo=T}
drop1(M0, scope = ~ Education, test = 'F')
```

# Nested normal linear models

```{r, echo=T}
drop1(M0, scope = ~ Education + Agriculture, test = 'F')
```

# Nested normal linear models

```{r, echo=T}
anova(M0)
```

# $R^2$

- If we have two models, $M_0$ and $M_1$, with $M_0$ nested in $M_1$, and with residual sums of squares $\text{RSS}_0$ and $\text{RSS}_1$, respectively, we can calculate:
$$
\begin{aligned}
\text{proportional decrease in error}&= \frac{\text{decrease in error (from $M_0$ to $M_1$)}}{\text{error in $M_0$}} ,\\
\\
&= \frac{\text{RSS}_0 - \text{RSS}_1}{\text{RSS}_0},\\
& = R^2
\end{aligned}
$$

```{r, echo=T}
(RSS_0 - RSS_1) / RSS_0
```

* In other words, the reduction in error from $M_0$ to $M_1$ is `r round((RSS_0 - RSS_1) / RSS_0, 2)` of the error of $M_0$. 


# $R^2$: The coefficient of determination

-   It can be shown that
    $$\underbrace{\sum_{i=1}^n (y_i-\bar{y})^2}_{\text{TSS}} = \underbrace{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}_{\text{ESS}} + \underbrace{\sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{RSS}},$$
    where TSS is *total* sum of squares, ESS is *explained* sum of
    squares, and RSS is *residual* sum of squares.

-   The coefficient of determination $R^2$ is defined as
    $$\begin{aligned}
    R^2 = \frac{\text{\footnotesize ESS}}{\text{\footnotesize TSS}} &= \text{\footnotesize Proportion of variation that is explained},\\
    &= 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2 }{\sum_{i=1}^n (y_i-\bar{y})^2 }\end{aligned}$$


# $R^2$

- If $M_0$ is a *null* model, i.e. no predictors, then $\text{TSS}$ = $\text{RSS}_0$.
-   It can be shown that
    $$\underbrace{\sum_{i=1}^n (y_i-\bar{y})^2}_{\text{RSS}_0} = \underbrace{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}_{\text{RSS}_0 - \text{RSS}_1} + \underbrace{\sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{RSS}_1}.$$

-   As such, $R^2$ is defined as
    $$\begin{aligned}
    R^2 = \frac{\text{RSS}_0-\text{RSS}_1}{\text{RSS}_0} = 1 - \frac{\text{RSS}_1}{\text{RSS}_0}
    \end{aligned},$$
    or 1 minus the error of $M_1$ relative to $M_0$.

# $R^2$

```{r, echo=T}
M_null <- lm(Fertility ~ 1, data = swiss)
RSS_null <- sum(residuals(M_null)^2)
RSS_0 / RSS_null
1 - RSS_0 / RSS_null
(RSS_null - RSS_0) / RSS_null
summary(M0)$r.squared
```


# Adjusted $R^2$

-   By explaining proportion of variance explained, $R^2$ is used a
    *goodness of fit* measure.

-   However, $R^2$ will always grow with $K$, the number of predictors.

-   $R^2$ can be *adjusted* to counteract the artificial effect of
    increasing numbers of predictors as follows:
    $$%R^2_{\text{Adj}} = 1 - (1-R^2) \frac{n-1}{n-K-1}
    R^2_{\text{Adj}}  = \underbrace{1-\frac{\text{RSS}}{\text{TSS}}}_{R^2}\underbrace{\frac{n-1}{n-K-1}}_{\text{penalty}},$$
    where $n$ is sample size.

-   $R^2_{\text{Adj}}$ is not identical to the proportion of
    variance explained in the *sample*, but is an unbiased measured of the population $R^2$.


# Adjusted $R^2$

```{r, echo=T}
n <- nrow(M0$model)
K <- length(coef(M0)) - 1 # no. of predictor coefs
penalty <- (n - 1)/(n - K - 1)
1 - (RSS_0 / RSS_null) * penalty
summary(M0)$adj.r.squared
```


# Deviance

-   The *deviance* of a model is defined
    $$-2 \log  L(\hat{\beta}\given\mathcal{D}) ,$$ where
    $\hat{\beta}$ are the mle estimates.
    
- The better the model fit, the *lower* the deviance.

- This can be seen as equivalent to RSS for generalized linear models.

# Deviance

```{r, echo=T}
swiss_df <- mutate(swiss, y = Fertility > median(Fertility))
M1 <- glm(y ~ Agriculture + Education + Catholic, 
          data = swiss_df, family = binomial())
M0 <- glm(y ~ Agriculture + Education, data = swiss_df, 
          family = binomial())

D_0 <- deviance(M0)
D_1 <- deviance(M1)

c(D_0, D_1)

(D_0 - D_1) / D_1 # prop. incr. error

(D_0 - D_1) / D_0 # equiv to R^2?
```



# Model comparison with deviance

-   Let us assume we have two models: $M_1$ and $M_0$ where $M_0$ is nested in $M_1$.
-   The deviance of $M_0$ minus the deviance of the $M_1$ is
    $$\Delta_{D} = D_0 - D_1.$$

-   Under the null hypothesis, $\Delta_D$ is distributed as $\chi^2$
    with $K_1 - K_0$ df, where $K_1$ is the number of parameters in $M_1$ and $K_0$ is the number of parameters in $M_0$.

# Model comparison with deviance

```{r, echo=T}
K_0 <- length(coef(M0))
K_1 <- length(coef(M1))

pchisq(D_0 - D_1, 
       df = K_1 - K_0,
       lower.tail = F)

anova(M0, M1, test = 'Chisq')
```

